# -*- coding: utf-8 -*-
""" 

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1LAbYysyqxA4JIbUbBlaWAhaC163d3Oua

# Mount Drive

# Importing Libraries
"""

from google.colab import drive
drive.mount('/content/drive')

!pip3 install texthero
!pip3 install transformers
!pip3 install tensorflow_addons

!pip install pyyaml==5.4.1


import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split

pip install -U spacy

import re
import nltk
from wordcloud import WordCloud
from nltk.stem import WordNetLemmatizer
from textblob import TextBlob,Word
from nltk.corpus import words
nltk.download('words')
nltk.download('wordnet')
#import texthero as hero       # to be learned
import re
#from texthero import stopwords

from nltk.corpus import wordnet #to be learned

import tensorflow as tf

from nltk.corpus import stopwords
from nltk.tokenize import TweetTokenizer

import tensorflow as tf

from sklearn.metrics import classification_report
from sklearn.metrics import accuracy_score,f1_score, confusion_matrix

training_data=pd.read_csv("/content/drive/MyDrive/CAPSTONE_19BCE1180/text_classification_data.csv",index_col=0)

import re
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from sklearn.model_selection import train_test_split

# Download NLTK resources
import nltk
nltk.download('stopwords')
nltk.download('punkt')



# Function to clean text
def clean_text(text):
    # Remove hyperlinks
    text = re.sub(r'https?:\/\/[^\s]*', '', text)
    # Remove hashtags
    text = re.sub(r'#\w+', '', text)
    # Remove usernames
    text = re.sub(r'@\w+', '', text)
    # Remove special characters and numbers
    text = re.sub(r'[^A-Za-z\s\']', '', text)  # Added \'
    # Convert to lowercase
    text = text.lower()
    # Tokenize the text
    tokens = word_tokenize(text)
    # Remove stopwords
    stop_words = set(stopwords.words('english'))
    tokens = [token for token in tokens if token not in stop_words and token.isalnum()]
    # Join tokens back into text
    cleaned_text = ' '.join(tokens)
    return cleaned_text

# Apply cleaning function to each row in the 'text' column
training_data['text_data'] = training_data['text_data'].apply(clean_text)

training_data['text_data']

#binning
age_group_list=[]
for i in training_data.age:
  if 0<=i<=24:
    age_group_list.append("xx-24")
  elif 25<=i<=34:
    age_group_list.append("25-34")
  elif 35<=i<=49:
    age_group_list.append("35-49")
  else:
    age_group_list.append("50-xx")
age_group_list=pd.Series(age_group_list)

training_data.insert(2,"age_group",age_group_list)

training_data

X_train, X_test, y_train, y_test = train_test_split(training_data[["userid","text_data"]], training_data["age_group"], test_size=0.1, random_state=42)

X_train

X_test

def lemma_per_pos(sent):
    '''function to lemmatize according to part of speech tag'''
    tokenizer=TweetTokenizer()
    lemmatizer = nltk.stem.WordNetLemmatizer()
    lemmatized_list = [lemmatizer.lemmatize(w) for w in  tokenizer.tokenize(sent)]
    return " ".join(lemmatized_list)

def df_preprocessing(df,feature_col):
    '''
    Preprocessing of dataframe
    '''
    stop = set(stopwords.words('english'))
    df[feature_col]= (df[feature_col].pipe(hero.lowercase).
                      pipe(hero.remove_urls).
                      pipe(hero.remove_digits).
                      pipe(hero.remove_punctuation).
                      pipe(hero.remove_html_tags) )
    # lemmatization
    df[feature_col]= [lemma_per_pos(sent) for sent in df[feature_col]]
    #df["word"]= hero.remove_stopwords(df["word"],stop)
    return df

from transformers import AutoTokenizer,TFDistilBertModel, DistilBertConfig
from transformers import TFAutoModel
import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Dense, Dropout
from transformers import AdamW, get_linear_schedule_with_warmup
import tensorflow_addons as tfa

"""# Reading Data

## ONE HOT ENCODING CLASS LABELS
"""

def one_hot(data):
  column_names = ["userid","text_data","xx-24","25-34","35-49","50-xx"]
  df = pd.DataFrame(columns = column_names)

  for i in range(len(data)):
    user_id=data["userid"].iloc[i]
    text_temp = data["text_data"].iloc[i]
    if data["age_group"].iloc[i] == "50-xx":
      df.loc[i] = [user_id,text_temp,0,0,0,1]
    elif data["age_group"].iloc[i] == "35-49":
      df.loc[i] = [user_id,text_temp,0,0,1,0]
    elif data["age_group"].iloc[i] == "25-34":
      df.loc[i] = [user_id,text_temp,0,1,0,0]
    elif data["age_group"].iloc[i] == "xx-24":
      df.loc[i] = [user_id,text_temp,1,0,0,0]
    else:
      df.loc[i]=[user_id,text_temp,0,0,0,0]
  return df

encoded_data=one_hot(training_data)

encoded_data

X_train, X_test= train_test_split(encoded_data, test_size=0, random_state=42)

X_train

encoded_data.to_csv("")



target_col= encoded_data.columns[2:]
feature_col=encoded_data.columns[1:2]

test

lid_dev

"""#Preprocessing"""

target_col,feature_col

"""### preprocessing training and testing data"""

import nltk
nltk.download('omw-1.4')

with tf.device('/GPU:0'):
 lid_tr_prp= df_preprocessing(lid_train,feature_col[0])

lid_dev_prp = df_preprocessing(lid_dev,feature_col[0])

#Creating tokenizer
def create_tokenizer(pretrained_weights='distilbert-base-uncased'):
  '''Function to create the tokenizer'''

  tokenizer = AutoTokenizer.from_pretrained(pretrained_weights)
  return tokenizer

#Tokenization of the data
def data_tokenization(dataset,feature_col,max_len,tokenizer):
    '''dataset: Pandas dataframe with feature name is column name
    Pretrained_weights: selected model
    RETURN: [input_ids, attention_mask]'''

    tokens = dataset[feature_col].apply(lambda x: tokenizer(x,return_tensors='tf',
                                                            truncation=True,
                                                            padding='max_length',
                                                            max_length=max_len,
                                                            add_special_tokens=True))
    input_ids= []
    attention_mask=[]
    for item in tokens:
        input_ids.append(item['input_ids'])
        attention_mask.append(item['attention_mask'])
    input_ids, attention_mask=np.squeeze(input_ids), np.squeeze(attention_mask)


    return [input_ids,attention_mask]

"""#Model"""

def bert_model(pretrained_weights,max_len,learning_rate):
  '''BERT model creation with pretrained weights
  INPUT:
  pretrained_weights: Language model pretrained weights
  max_len: input length '''
  print('Model selected:', pretrained_weights)
  bert=TFAutoModel.from_pretrained(pretrained_weights)

  # This is must if you would like to train the layers of language models too.
  for layer in bert.layers:
      layer.trainable = True

  # # parameter declaration
  # step = tf.Variable(0, trainable=False)
  # schedule = tf.optimizers.schedules.PiecewiseConstantDecay([10000, 15000], [2e-0, 2e-1, 1e-2])
  # # lr and wd can be a function or a tensor
  # lr = learning_rate * schedule(step)
  # wd = lambda:lr * schedule(step)
  # optimizer = tfa.optimizers.AdamW(learning_rate=lr, weight_decay=wd)

  # optimizer= tf.keras.optimizers.Adam(learning_rate=learning_rate, beta_1=0.9, beta_2=0.999, epsilon=1e-07, amsgrad=False,name='Adam')
  optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate)

  # declaring inputs, BERT take input_ids and attention_mask as input
  input_ids= Input(shape=(max_len,),dtype=tf.int32,name='input_ids')
  attention_mask=Input(shape=(max_len,),dtype=tf.int32,name='attention_mask')

  bert= bert(input_ids,attention_mask=attention_mask)
  x= bert[0][:,0,:]
  x=tf.keras.layers.Dropout(0.05)(x)
  x= tf.keras.layers.Dense(128)(x)
  x=tf.keras.layers.Dense(64)(x)
  x=tf.keras.layers.Dense(32)(x)

  output=tf.keras.layers.Dense(4,activation='relu')(x)

  model=Model(inputs=[input_ids,attention_mask],outputs=[output])
  # compiling model
  model.compile(optimizer=optimizer,
                loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True, reduction=tf.keras.losses.Reduction.NONE,name='categorical_crossentropy'),
                metrics=['accuracy'])
  return model

pretrained_weights='distilbert-base-uncased'#'bert-base-uncased'
max_len=256
epochs=3   # change epochs here
learning_rate=2e-5
batch_size=4

tokenizer= create_tokenizer(pretrained_weights)

tokenizer.save_pretrained('/content/drive/MyDrive/UWT_ML_PROJECT_MODEL/Tokenizer')

x_full_train_tokens=data_tokenization(encoded_data,feature_col[0],max_len,tokenizer)

x_train_tokens= data_tokenization(X_train,feature_col[0],max_len,tokenizer)

x_test_tokens=data_tokenization(X_test,feature_col[0],max_len,tokenizer)

x_train = np.asarray(x_train).astype(np.float32)

y_train= encoded_data[target_col].values
y_train

y_train.shape

y_test=X_test[target_col].values
y_test

y_test.shape

bert=bert_model(pretrained_weights,max_len,learning_rate)
bert.summary()

class_weights = compute_class_weight('balanced', classes=range(4), y=y_train.argmax(axis=1))
class_weight_dict = {i: weight for i, weight in enumerate(class_weights)}


# GPU TRAINING
with tf.device('/GPU:0'):
    bert.fit(x_full_train_tokens,y_train,batch_size=batch_size,epochs=epochs,verbose=1)

bert.save("/content/drive/MyDrive/UWT_ML_PROJECT_MODEL/Model")

preds= bert.predict(x_test_tokens)

preds

preds.shape
predicted_test_labels=pd.DataFrame(preds)

predicted_test_labels.columns=target_col

predicted_test_labels

predicted_test_labels["50-xx"].value_counts()

pred_labels=[]
for i in range(predicted_test_labels.shape[0]):
  temp = max(predicted_test_labels['xx-24'].iloc[i],predicted_test_labels['25-34'].iloc[i],predicted_test_labels['35-49'].iloc[i],predicted_test_labels['50-xx'].iloc[i])
  if temp == predicted_test_labels['xx-24'].iloc[i]:
    pred_labels.append("xx-24")
  elif temp == predicted_test_labels['25-34'].iloc[i]:
    pred_labels.append("25-34")
  elif temp == predicted_test_labels['35-49'].iloc[i]:
    pred_labels.append("35-49")
  elif temp == predicted_test_labels['50-xx'].iloc[i]:
    pred_labels.append("50-xx")

test_labels=[]
for i in range(X_test.shape[0]):
  temp = max(X_test['xx-24'].iloc[i],X_test['25-34'].iloc[i],X_test['35-49'].iloc[i],X_test['50-xx'].iloc[i])
  if temp == X_test['xx-24'].iloc[i]:
    test_labels.append("xx-24")
  elif temp == X_test['25-34'].iloc[i]:
    test_labels.append("25-34")
  elif temp == X_test['35-49'].iloc[i]:
    test_labels.append("35-49")
  elif temp == X_test['50-xx'].iloc[i]:
    test_labels.append("50-xx")

from transformers import DistilBertTokenizer, TFDistilBertModel

# Assuming tokenizer and model are already created
tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')
model = TFDistilBertModel.from_pretrained('distilbert-base-uncased')

# Save the tokenizer and model to a directory
save_directory = '/content/drive/MyDrive/CAPSTONE_19BCE1180'
tokenizer.save_pretrained(save_directory)
model.save_pretrained(save_directory)

"""## performance on dev data"""

print(classification_report(test_labels,pred_labels))

print(target_col)

pred_labels

from sklearn.metrics import confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt
import pandas as pd

# Assuming 'y_true' is the true labels and 'y_pred' is the predicted labels
y_true = test_labels  
y_pred = pred_labels  
classes=['xx-24', '25-34',  '50-xx','35-49']


print(cl_report)




##############################################################################
#USING SVM TO PREDICT AGE INTERVALS
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import normalize

scaler=StandardScaler(with_mean=False)
vectorizer1 = TfidfVectorizer(ngram_range = (1,1))
tfidf_vectors=scaler.fit_transform(vectorizer1.fit_transform(training_data["text_data"]))

X_train, X_test, y_train, y_test = train_test_split(tfidf_vectors, training_data["age_group"], test_size=0.2, random_state=42)

from sklearn.svm import SVC
svclassifier = SVC(kernel='linear', gamma="auto")
svclassifier.fit(X_train,y_train)

preds=svclassifier.predict(X_test)

print(classification_report(y_test,preds))
##############################################################################
